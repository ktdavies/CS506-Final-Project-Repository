{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets explicitly\n",
    "airports = pd.read_csv('/Users/michael/Desktop/CS506 Project/Test/Airports.csv', skiprows=1)\n",
    "flights = pd.read_csv('/Users/michael/Desktop/CS506 Project/Test/Flights.csv')\n",
    "weather = pd.read_csv('/Users/michael/Desktop/CS506 Project/Test/Weather.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge Step 1: Clean and Prepare Airports and Weather Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explicitly clean and prepare Airports DataFrame to map ICAO to IATA clearly\n",
    "airport_codes = airports[['icao', 'code']].copy()\n",
    "airport_codes.columns = ['ICAO', 'IATA']\n",
    "airport_codes['ICAO'] = airport_codes['ICAO'].str.strip().str.upper()\n",
    "airport_codes['IATA'] = airport_codes['IATA'].str.strip().str.upper()\n",
    "\n",
    "# Clean Weather dataset explicitly and prepare ICAO and date columns\n",
    "weather['AirportCode'] = weather['AirportCode'].str.strip().str.upper()\n",
    "weather['Date'] = pd.to_datetime(weather['StartTime(UTC)']).dt.date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge Step 2: Merge Weather and Airports for IATA Codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge weather explicitly with airports to get IATA codes\n",
    "weather_with_iata = weather.merge(\n",
    "    airport_codes,\n",
    "    left_on='AirportCode',\n",
    "    right_on='ICAO',\n",
    "    how='left'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge Step 3: Clean Flights Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean flights dataset explicitly\n",
    "flights['ORIGIN'] = flights['ORIGIN'].str.strip().str.upper()\n",
    "flights['DEST'] = flights['DEST'].str.strip().str.upper()\n",
    "flights['Date'] = pd.to_datetime(flights['FL_DATE']).dt.date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge Step 4: Merge Weather and Flight Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge weather data explicitly with flights data based on ORIGIN airport and Date\n",
    "merged_origin_weather = flights.merge(\n",
    "    weather_with_iata,\n",
    "    left_on=['ORIGIN', 'Date'],\n",
    "    right_on=['IATA', 'Date'],\n",
    "    how='left',\n",
    "    suffixes=('', '_origin_weather')\n",
    ").rename(columns={\n",
    "    'Type': 'WeatherType_Origin',\n",
    "    'Severity': 'Severity_Origin',\n",
    "    'Precipitation(in)': 'Precipitation_Origin'\n",
    "})\n",
    "\n",
    "# Merge weather data explicitly again with flights data for DEST airport\n",
    "merged_full_weather = merged_origin_weather.merge(\n",
    "    weather_with_iata,\n",
    "    left_on=['DEST', 'Date'],\n",
    "    right_on=['IATA', 'Date'],\n",
    "    how='left',\n",
    "    suffixes=('', '_dest_weather')\n",
    ").rename(columns={\n",
    "    'Type': 'WeatherType_Dest',\n",
    "    'Severity': 'Severity_Dest',\n",
    "    'Precipitation(in)': 'Precipitation_Dest'\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge Step 5: Clean and Finalise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/z1/_2fxlcv92cz685h4m3hztvx80000gn/T/ipykernel_39983/1985234846.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  final_dataset[col] = pd.to_datetime(final_dataset[col], errors='coerce').dt.time\n",
      "/var/folders/z1/_2fxlcv92cz685h4m3hztvx80000gn/T/ipykernel_39983/1985234846.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  final_dataset[col] = pd.to_datetime(final_dataset[col], errors='coerce').dt.time\n",
      "/var/folders/z1/_2fxlcv92cz685h4m3hztvx80000gn/T/ipykernel_39983/1985234846.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  final_dataset[col] = pd.to_datetime(final_dataset[col], errors='coerce').dt.time\n",
      "/var/folders/z1/_2fxlcv92cz685h4m3hztvx80000gn/T/ipykernel_39983/1985234846.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  final_dataset[col] = pd.to_datetime(final_dataset[col], errors='coerce').dt.time\n",
      "/var/folders/z1/_2fxlcv92cz685h4m3hztvx80000gn/T/ipykernel_39983/1985234846.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  final_dataset[categorical_fill] = final_dataset[categorical_fill].fillna('None')\n",
      "/var/folders/z1/_2fxlcv92cz685h4m3hztvx80000gn/T/ipykernel_39983/1985234846.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  final_dataset[numerical_zero_fill] = final_dataset[numerical_zero_fill].fillna(0)\n",
      "/var/folders/z1/_2fxlcv92cz685h4m3hztvx80000gn/T/ipykernel_39983/1985234846.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  final_dataset[time_cols_fill] = final_dataset[time_cols_fill].fillna(-1)\n",
      "/var/folders/z1/_2fxlcv92cz685h4m3hztvx80000gn/T/ipykernel_39983/1985234846.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  final_dataset[time_weather_fill] = final_dataset[time_weather_fill].fillna('None')\n",
      "/var/folders/z1/_2fxlcv92cz685h4m3hztvx80000gn/T/ipykernel_39983/1985234846.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  final_dataset['DELAYED'] = (final_dataset['ARR_DELAY'] >= 15).astype(int)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully merged Flights and Weather datasets. Here's a preview:\n",
      "      FL_DATE                AIRLINE          ORIGIN_CITY    DEST_CITY  \\\n",
      "0  2019-01-09  United Air Lines Inc.  Fort Lauderdale, FL   Newark, NJ   \n",
      "1  2022-11-19   Delta Air Lines Inc.      Minneapolis, MN  Seattle, WA   \n",
      "2  2022-11-19   Delta Air Lines Inc.      Minneapolis, MN  Seattle, WA   \n",
      "3  2022-11-19   Delta Air Lines Inc.      Minneapolis, MN  Seattle, WA   \n",
      "4  2022-11-19   Delta Air Lines Inc.      Minneapolis, MN  Seattle, WA   \n",
      "\n",
      "   CRS_DEP_TIME  DEP_TIME  DEP_DELAY  CRS_ARR_TIME  ARR_TIME  ARR_DELAY  ...  \\\n",
      "0          1155    1151.0       -4.0          1501    1447.0      -14.0  ...   \n",
      "1          2120    2114.0       -6.0          2315    2310.0       -5.0  ...   \n",
      "2          2120    2114.0       -6.0          2315    2310.0       -5.0  ...   \n",
      "3          2120    2114.0       -6.0          2315    2310.0       -5.0  ...   \n",
      "4          2120    2114.0       -6.0          2315    2310.0       -5.0  ...   \n",
      "\n",
      "  Severity_Origin  StartTime(UTC)  EndTime(UTC)  Precipitation_Origin  \\\n",
      "0            None            None          None                   0.0   \n",
      "1           Light        09:28:00      10:53:00                   0.0   \n",
      "2           Light        11:15:00      12:53:00                   0.0   \n",
      "3           Light        15:44:00      16:35:00                   0.0   \n",
      "4           Light        16:41:00      17:04:00                   0.0   \n",
      "\n",
      "   WeatherType_Dest  Severity_Dest  StartTime(UTC)_dest_weather  \\\n",
      "0              Rain          Light                     03:37:00   \n",
      "1              None           None                         None   \n",
      "2              None           None                         None   \n",
      "3              None           None                         None   \n",
      "4              None           None                         None   \n",
      "\n",
      "   EndTime(UTC)_dest_weather Precipitation_Dest DELAYED  \n",
      "0                   06:51:00                0.4       0  \n",
      "1                       None                0.0       0  \n",
      "2                       None                0.0       0  \n",
      "3                       None                0.0       0  \n",
      "4                       None                0.0       0  \n",
      "\n",
      "[5 rows x 29 columns]\n",
      "\n",
      "Final merged dataset dimensions: (11852084, 29)\n"
     ]
    }
   ],
   "source": [
    "# Keep only relevant columns\n",
    "columns_to_keep = ['FL_DATE', 'AIRLINE', 'ORIGIN_CITY', 'DEST_CITY', 'CRS_DEP_TIME', 'DEP_TIME', 'DEP_DELAY', 'CRS_ARR_TIME', 'ARR_TIME', 'ARR_DELAY', 'CANCELLATION_CODE', 'AIR_TIME', 'DISTANCE', 'DELAY_DUE_CARRIER', 'DELAY_DUE_WEATHER', 'DELAY_DUE_NAS', 'DELAY_DUE_SECURITY', 'DELAY_DUE_LATE_AIRCRAFT', 'WeatherType_Origin', 'Severity_Origin', 'StartTime(UTC)', 'EndTime(UTC)', 'Precipitation_Origin', 'WeatherType_Dest', 'Severity_Dest', 'StartTime(UTC)_dest_weather', 'EndTime(UTC)_dest_weather', 'Precipitation_Dest']\n",
    "final_dataset = merged_full_weather[columns_to_keep]\n",
    "time_columns = ['StartTime(UTC)', 'EndTime(UTC)', 'StartTime(UTC)_dest_weather', 'EndTime(UTC)_dest_weather']\n",
    "\n",
    "# Fix time formatting\n",
    "for col in time_columns:\n",
    "    final_dataset[col] = pd.to_datetime(final_dataset[col], errors='coerce').dt.time\n",
    "\n",
    "# Fill text/categorical columns\n",
    "categorical_fill = ['CANCELLATION_CODE', 'WeatherType_Origin', 'Severity_Origin', 'WeatherType_Dest', 'Severity_Dest']\n",
    "final_dataset[categorical_fill] = final_dataset[categorical_fill].fillna('None')\n",
    "\n",
    "# Fill numerical delay reasons and precipitation\n",
    "numerical_zero_fill = ['DELAY_DUE_LATE_AIRCRAFT', 'DELAY_DUE_CARRIER', 'DELAY_DUE_WEATHER', 'DELAY_DUE_NAS', 'DELAY_DUE_SECURITY', 'Precipitation_Origin', 'Precipitation_Dest']\n",
    "final_dataset[numerical_zero_fill] = final_dataset[numerical_zero_fill].fillna(0)\n",
    "\n",
    "# Fill AIR_TIME, ARR_TIME, DEP_TIME, ARR_DELAY, DEP_DELAY with -1 to flag missing\n",
    "time_cols_fill = ['AIR_TIME', 'ARR_TIME', 'DEP_TIME', 'ARR_DELAY', 'DEP_DELAY']\n",
    "final_dataset[time_cols_fill] = final_dataset[time_cols_fill].fillna(-1)\n",
    "\n",
    "# Fill StartTime(UTC) and EndTime(UTC) weather columns with 'None'\n",
    "time_weather_fill = ['StartTime(UTC)', 'EndTime(UTC)', 'StartTime(UTC)_dest_weather', 'EndTime(UTC)_dest_weather']\n",
    "final_dataset[time_weather_fill] = final_dataset[time_weather_fill].fillna('None')\n",
    "\n",
    "# Create binary delay label\n",
    "final_dataset['DELAYED'] = (final_dataset['ARR_DELAY'] >= 15).astype(int)\n",
    "\n",
    "# Save explicitly the final cleaned merged dataset\n",
    "final_dataset.to_csv('final_flights_weather_merged.csv', index=False)\n",
    "\n",
    "# Explicitly verify the merged dataset clearly\n",
    "print(\"✅ Successfully merged Flights and Weather datasets. Here's a preview:\")\n",
    "print(final_dataset.head())\n",
    "print(f\"\\nFinal merged dataset dimensions: {final_dataset.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Step 1: Split into Training and Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/z1/_2fxlcv92cz685h4m3hztvx80000gn/T/ipykernel_39983/3975593277.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  final_dataset['FL_DATE'] = pd.to_datetime(final_dataset['FL_DATE'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (8896833, 29)\n",
      "Test 1 shape: (1662195, 29)\n",
      "Test 2 shape: (1662195, 29)\n"
     ]
    }
   ],
   "source": [
    "# Make sure FL_DATE is datetime\n",
    "final_dataset['FL_DATE'] = pd.to_datetime(final_dataset['FL_DATE'])\n",
    "\n",
    "# Define your splits\n",
    "train_mask = (\n",
    "    ((final_dataset['FL_DATE'] >= '2019-01-01') & (final_dataset['FL_DATE'] <= '2020-03-31')) |\n",
    "    ((final_dataset['FL_DATE'] >= '2021-01-01') & (final_dataset['FL_DATE'] <= '2022-06-30'))\n",
    ")\n",
    "\n",
    "test_mask = (\n",
    "    (final_dataset['FL_DATE'] > '2022-06-30') & (final_dataset['FL_DATE'] <= '2023-06-30')\n",
    ")\n",
    "\n",
    "# Split\n",
    "train_data = final_dataset[train_mask].reset_index(drop=True)\n",
    "test_data_1 = final_dataset[test_mask].reset_index(drop=True)\n",
    "test_data_2 = final_dataset[test_mask].reset_index(drop=True)\n",
    "\n",
    "print(f\"Train shape: {train_data.shape}\")\n",
    "print(f\"Test 1 shape: {test_data_1.shape}\")\n",
    "print(f\"Test 2 shape: {test_data_2.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Step 2: Adjust Test 2 for Climate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Parameters\n",
    "future_precip_increase = 1.10  # +10% precipitation overall\n",
    "heavy_precip_threshold = 0.3   # Threshold for heavy precipitation in inches\n",
    "heavy_event_intensity_increase = 1.10  # +10% more rain on heavy days\n",
    "frequency_boost_percent = 16  # +16% more rainy days\n",
    "new_precip_range = (0.05, 0.1)  # Inches for new light rain days\n",
    "\n",
    "def remap_severity(df, precip_col, severity_col):\n",
    "    \"\"\"\n",
    "    Remaps severity based on precipitation values.\n",
    "    \"\"\"\n",
    "    conditions = [\n",
    "        (df[precip_col].isna()) | (df[precip_col] == 0),\n",
    "        (df[precip_col] > 0) & (df[precip_col] <= 0.1),\n",
    "        (df[precip_col] > 0.1) & (df[precip_col] <= 0.5),\n",
    "        (df[precip_col] > 0.5) & (df[precip_col] <= 1.0),\n",
    "        (df[precip_col] > 1.0)\n",
    "    ]\n",
    "    choices = [np.nan, 'Light', 'Moderate', 'Heavy', 'Severe']\n",
    "    \n",
    "    df[severity_col] = np.select(conditions, choices, default=np.nan)\n",
    "\n",
    "# Adjust Precipitation on test_data_2\n",
    "adjusted_test_data_2 = test_data_2.copy()\n",
    "\n",
    "# Apply +10% to all precipitation values\n",
    "for col in ['Precipitation_Origin', 'Precipitation_Dest']:\n",
    "    if col in adjusted_test_data_2.columns:\n",
    "        adjusted_test_data_2[col] = adjusted_test_data_2[col] * future_precip_increase\n",
    "\n",
    "# Further increase heavy precipitation events\n",
    "for col in ['Precipitation_Origin', 'Precipitation_Dest']:\n",
    "    if col in adjusted_test_data_2.columns:\n",
    "        heavy_mask = adjusted_test_data_2[col] > heavy_precip_threshold\n",
    "        adjusted_test_data_2.loc[heavy_mask, col] *= heavy_event_intensity_increase\n",
    "\n",
    "# Adjust Precipitation Frequency \n",
    "for col in ['Precipitation_Origin', 'Precipitation_Dest']:\n",
    "    if col in adjusted_test_data_2.columns:\n",
    "        dry_mask = adjusted_test_data_2[col] == 0.0\n",
    "        dry_indices = adjusted_test_data_2[dry_mask].index\n",
    "\n",
    "        # How many dry days to convert to light rain?\n",
    "        num_to_convert = int(len(dry_indices) * (frequency_boost_percent / 100))\n",
    "\n",
    "        if num_to_convert > 0:\n",
    "            selected_indices = np.random.choice(dry_indices, size=num_to_convert, replace=False)\n",
    "            adjusted_test_data_2.loc[selected_indices, col] = np.random.uniform(new_precip_range[0], new_precip_range[1], size=num_to_convert)\n",
    "\n",
    "# Apply remapping to Severity columns\n",
    "remap_severity(adjusted_test_data_2, 'Precipitation_Origin', 'Severity_Origin')\n",
    "remap_severity(adjusted_test_data_2, 'Precipitation_Dest', 'Severity_Dest')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
